# Design Decisions & Rationale
<!-- Note, these decisions were taken by agent AI systems, this is just for us to be able to understand -->
- Established a multi-backend architecture (Groq hosted 70B `llama3-3-70b-versatile`, self-hosted vLLM, local HF) with a shared `BaseClassifier` interface to enforce identical I/O and decoding parameters across models for fair comparisons.
- Switched prompting to a JSON-based contract (`MISINFO_SYSTEM_PROMPT` + serialized user payload) to reduce formatting drift across providers and make parsing resilient.
- Added deterministic decoding defaults (`temperature=0.0`, `top_p=1.0`, fixed max tokens) to keep outputs comparable between large and small models.
- Introduced structured configuration (`config/base.yaml` + per-model YAMLs) so runs are reproducible and swappable without code changes.
- Added dataset split handling (`split` column with filter in config) so evaluation/test subsets stay reproducible and clean.
- Output schema standardized to JSONL with metadata (id, claim/context, backend, model, latency, timestamp, raw + parsed output) for downstream scoring and audits.
- Provided SOPs and README to make environment setup, runs, and extension to new models repeatable without tribal knowledge.
- Default CLI target is Groq 70B (`llama3-3-70b-versatile`) for convenience; swap backends by pointing `--model-config` to the desired YAML.
- Installed CPU Torch wheels by default to avoid GPU-specific constraints during setup; adjust the pip index if you want CUDA builds.
- Selected two automated metrics to add: (1) Fact Precision via retrieval + NLI (with broken/empty evidence treated as unsupported, refuted claims driving score down, optional small human spot-check hook); (2) Self-Consistency risk via multi-sample generation and lexical/NLI disagreement scoring. Will implement modular evaluators and config toggles, integrating into the runner.
- Tightened metric definitions (in-progress implementation): Fact Precision now uses per-sentence claim units, top-k evidence aggregation, thresholded NLI decisions (entail/contradict margins), and logs refute_rate and coverage separately from fact_precision. Self-Consistency will incorporate sentence-embedding similarity (optionally NLI contradictions) with dedicated decoding parameters. Retrieval backend is configurable (default: context-only; planned: Wikipedia BM25) with unsupported when retrieval fails or evidence is empty.
- BM25 corpus built from a curated Wikipedia seed list (~14.6k passages in `data/wiki_passages.tsv`); retrieval backend supports `min_score` filtering to drop weak hits. Added probe + calibration scaffolding scripts.
- Added claim-level retrieval option for Fact Precision (`query_source: claim`) to reuse evidence across rationale sentences, and optional `claim_verification` + `label_consistency` metrics to keep model-rationale scoring separate from dataset-claim checks. Configurable via `metrics.fact_precision.retrieval.query_source`, `metrics.claim_verification.enabled`, and `metrics.label_consistency.enabled`.
- Added LIAR dataset plumbing: `scripts/build_liar_topics.py` to derive topics from claims, built a LIAR-focused corpus (`data/liar_passages.tsv`, ~3.5k passages) and set it as default; switched default NLI to public `MoritzLaurer/deberta-v3-base-mnli-fever-anli` with thresholds 0.4/0.4; bumped top_k (12) and max_evidence (5) for better coverage. Fact Precision, Claim Verification, and Label Consistency now use claim-level retrieval over the LIAR corpus by default.
- Split execution into two phases: generation-only (Groq/vLLM/HF) writes JSONL; offline evaluation reuses a shared retrieval+NLI cache (batched, two-stage NLI) so metric tweaks do not re-query Groq and thresholds/evidence stay consistent across fact_precision/claim_verification/label_consistency.
- Aggregation tightened: claim_verification/label_consistency now use max entail vs max contradict with a small margin to reduce spurious “mixed” from weak contradictions; margin and thresholds are configurable in `config/eval_liar.yaml`.
